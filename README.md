# Supervised-Learning-Linear-Regression

This project is for the course of Supervised Learning: Regression, which is part of IBM Machine Learning Professional Certificate by Coursera.
https://www.coursera.org/account/accomplishments/verify/KLV4XJBDEMA6?utm_source=ln&utm_medium=certificate&utm_content=cert_image&utm_campaign=pdf_header_button&utm_product=course

Main objective of the analysis that specifies whether your model will be focused on prediction or interpretation.
Main objective of this analysis is prediction on the dataset which is about used cars pricing.

Brief description of the data set you chose and a summary of its attributes.
Dataset that u used in this project was acquired from Kaggle which was published from Nehel Birla, with the name car details v3.csv. url: https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho?select=Car+details+v3.csv this dataset has 13 columns and 8128 entries. Describes car characteristics, year that certain car was sold, name and the selling price.

Brief summary of data exploration and actions taken for data cleaning and feature engineering.
7907 entries have no null values from 8128 in the dataset, the first step is to identify these null values and delete them, after that the whole dataset has no null values. The dataset contains 1 float, 3 integer and 9 strings data types. The columns mileage, engine and max power have strings and special characters which will be replaced with whitespace in order to transform these columns into numeric. Dropping the column name which has unique name and it will ruin my initial model. The column owner has 5 values: • First Owner:5215 • Second Owner :2016 • Other Owner:510 • Fourth & Above Owner:160 • Test Drive Car:5 Except of first and second owner, the rest have small amount of entries, so I am adding the rest in one category with the name Other Owner. Also, the column Seller type has 3 values: • Individual:6563 • Dealer:1107 • Trustmark Dealer:236 I am adding the Trustmark dealer in the same category with dealer in order to not to be seen as an outlier. Dropping the CNG and LPG entries because i want to deal only with diesel and petrol, and because in this dataset they have insufficient number of entries. Only cars with 5 or 7 seats have enough entries, I am putting the rest in a category 1(seats:8,4,9,6,10,2,14) From the scatter plot I can see that the car with max power 400 is an outlier with only 1 entry, so I am deleting it. Add new column with binary values of 1 and 0 from fuel, seller type and transmission, and add new column with values of 0,1,2 from owner, using OrdinalEncoder from sklearn. And finally, in description table of the dataset I am adding range and median values. Separated the dataset to two new ones X and y, with X being all numeric values and y being target value for prediction which is the selling price. After applying normaltest, from scipy, I can see that it is not normally distributed. I tried to transform them with boxcox, log and log1p, with the boxcox having the lowest pvalue, even though its still far from normally distributed, but that will not be a problem because linear algorithms do not need normally distributed data.

Summary of training at least three linear regression models which should be variations that cover using a simple linear regression as a baseline, adding polynomial effects, and using a regularization regression. Preferably, all use the same training and test splits, or the same cross-validation method.
I trained 3 linear regression models. Vanilla Linear Regression, Ridge Regression and Lasso. For Linear regression model I used boxcox transformed data. Fitted om Polynomial Features with degree=3. Split data with train_test_split with test size of 30%. then fit Linear Regression algorithm and made the prediction, but in order to make the prediction make sense I invert transformed data from boxcox, in order to be in the same scale as the test data. For Ridge Regression I used GridSearchCV, to find the best parameters for the model to be balanced in bias and variance, and pipeline, which helps in job distribution of the model for faster results. The GridSearchCV gave the best score with polynomial features of 3 and the alpha of the regression of 1.0, and applied these parameters on the model to get the R2 score of the prediction model, and coefficiants. For Lasso Regression I used pipeline again like in Ridde Regression,for the same reasons as above. And for predictions in Lasso model I Used cross_val_predict, and then fitted the model on my data to get the R2 score and coeficients.

A paragraph explaining which of your regressions you recommend as a final model that best fits your needs in terms of accuracy and explainability.
Ridge and Lasso Regression both performed on the same level in concern of R2, with approximately of 90, and Linear regression had lower R2 of 84, which means that the 2 first that I am mentioning have better predictions on my data set. So, Lasso and Ridge Regression are better models for my dataset.

Summary Key Findings and Insights, which walks your reader through the main drivers of your model and insights from your data derived from your linear regression model.
Linear Regression Magnitude: 80.24933829485636 Number of coefficients not equal to 0 for Linear 219 Lasso Regression Magnitude: 97762.65016846356 Number of coefficients not equal to 0 for Lasso 219 Ridged Regression Magnitude: 638963.4688658034 Number of coefficients not equal to 0 for Ridge 219 Usually in Lasso Regression some coefficients turn to 0 but in my model none were turned into 0, in any of my regression models. The Linear Regression has very lo magnitude which means that it has no complexity, and the Ridge Regression has very high magnitude which means it is probably to complex. Linear Regression R2 Score: 0.8455083009369075 Ridge Regression R2 Score: 0.9078627327434854 Lasso Regression R2 Score: 0.9078711849048965 Ridge and Lasso Regression both performed on the same level in concern of R2, with approximately of 90, and Linear regression had lower R2 of 84, which means that the 2 first that I am mentioning have better predictions on my data set.

Suggestions for next steps in analyzing this data, which may include suggesting revisiting this model adding specific data features to achieve a better explanation or a better prediction.
Next, I want to apply also ElasticNet model on this dataset in order to see results from that model too. Also, I want to do interpretation analysis. And finally make more visualization to all the results to be able to get more insights.
